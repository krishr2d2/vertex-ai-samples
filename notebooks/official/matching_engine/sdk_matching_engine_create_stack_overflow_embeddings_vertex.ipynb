{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Using Vertex AI Matching Engine and Vertex AI Embeddings for Text for StackOverflow Questions \n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/matching_engine/sdk_matching_engine_create_stack_overflow_embeddings_vertex.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/matching_engine/sdk_matching_engine_create_stack_overflow_embeddings_vertex.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "      <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/matching_engine/sdk_matching_engine_create_stack_overflow_embeddings_vertex\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_NOTE_**: This notebook has been tested in the following environment:\n",
    "\n",
    "* Python version = 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0a74aaf1481"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This example demonstrates how to encode text as embeddings using the Vertex AI Embeddings for Text service and the StackOverflow dataset. The embeddings are uploaded to the Vertex AI Matching Engine service, which is a high-scale, low-latency solution, for finding similar vectors from a large corpus. Matching Engine is a fully managed offering, further reducing operational overhead. It is built upon [Approximate Nearest Neighbor (ANN) technology](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html) developed by Google Research.\n",
    "\n",
    "Learn more about [Vertex AI Matching Engine](https://cloud.google.com/vertex-ai/docs/matching-engine/overview) and [Vertex AI Embeddings for Text](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34a4b245e795"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this notebook, you learn how to encode text as embeddings, create an Approximate Nearest Neighbor (ANN) index, and query against indexes.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- `Vertex AI Matching Engine`\n",
    "- `Vertex AI Embeddings for Text`\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "* Convert a BigQuery dataset to embeddings\n",
    "* Create an index\n",
    "* Upload embeddings to the index\n",
    "* Create an index endpoint\n",
    "* Deploy the index to the index endpoint\n",
    "* Perform an online query\n",
    "* Add metadata to a Redis store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used for this tutorial is the [StackOverflow dataset](https://console.cloud.google.com/marketplace/product/stack-exchange/stack-overflow).\n",
    "\n",
    "> Stack Overflow is the largest online community for programmers to learn, share their knowledge, and advance their careers. Updated on a quarterly basis, this BigQuery dataset includes an archive of Stack Overflow content, including posts, votes, tags, and badges. This dataset is updated to mirror the Stack Overflow content on the Internet Archive, and is also available through the Stack Exchange Data Explorer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* BigQuery\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [BigQuery pricing](https://cloud.google.com/bigquery/pricing), and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), \n",
    "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0f1bea346db"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the latest version of Cloud Storage, BigQuery, and the Vertex AI SDK for Python. Also install the latest version of Redis for low-latency data retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dfbccc635a17"
   },
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "! pip3 install --upgrade google-cloud-aiplatform \\\n",
    "                        google-cloud-storage \\\n",
    "                        'google-cloud-bigquery[pandas]' \\\n",
    "                        redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b08ba354c6e"
   },
   "source": [
    "### Colab only: Uncomment the following cell to restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bea801acf6b5"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "3. [Enable the Vertex AI and Redis APIs](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,redis.googleapis.com).\n",
    "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd28c9e4f067"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "If you don't know your project ID, try the following:\n",
    "* Run `gcloud config list`\n",
    "* Run `gcloud projects list`\n",
    "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80c0215f05a0"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[YOUR-PROJECT-ID]\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4f4512bf63b3"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "474be5183c27"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "949271bfebe3"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b65b4ce80d9a"
   },
   "source": [
    "**1. Vertex AI Workbench**\n",
    "* Do nothing as you are already authenticated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "985cdbfe7372"
   },
   "source": [
    "**2. Local JupyterLab instance, uncomment and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbc9cd30cc4b"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79efab26ad02"
   },
   "source": [
    "**3. Colab, uncomment and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a336a05c6149"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c0a44fa330f"
   },
   "source": [
    "**4. Service account or other**\n",
    "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3uj8x73nDX_"
   },
   "source": [
    "* Authentication: Rerun the `gcloud auth login` command in the Vertex AI Workbench notebook terminal when you are logged out and need the credential again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "Create a storage bucket to store intermediate artifacts such as datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import redis\n",
    "import random\n",
    "import tempfile\n",
    "import functools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Any, Generator, List, Tuple, Optional\n",
    "from vertexai.preview.language_models import TextEmbeddingModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, \n",
    "                location=REGION, \n",
    "                staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lR6Wwv-hCCN-"
   },
   "source": [
    "## Prepare the data\n",
    "\n",
    "You use the [Stack Overflow dataset](https://console.cloud.google.com/marketplace/product/stack-exchange/stack-overflow) of question and answers hosted on BigQuery.\n",
    "\n",
    "> This public dataset is hosted in Google BigQuery and is included in BigQuery's 1TB/mo of free tier processing. This means that each user receives 1TB of free BigQuery processing every month, which can be used to run queries on this public dataset.\n",
    "\n",
    "The BigQuery table is too large to fit into memory, so you need to write a generator called `query_bigquery_chunks` to yield chunks of the dataframe for processing. Additionally, an extra column `title_with_body` is added, which is a concatenation of the question title and body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ed1b3f87c475"
   },
   "outputs": [],
   "source": [
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "QUERY_TEMPLATE = \"\"\"\n",
    "        SELECT distinct q.id, q.title, q.body\n",
    "        FROM (SELECT * FROM `bigquery-public-data.stackoverflow.posts_questions` where Score>0 ORDER BY View_Count desc) AS q \n",
    "        LIMIT {limit} OFFSET {offset};\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "def query_bigquery_chunks(\n",
    "    max_rows: int, rows_per_chunk: int, start_chunk: int = 0\n",
    ") -> Generator[pd.DataFrame, Any, None]:\n",
    "    for offset in range(start_chunk, max_rows, rows_per_chunk):\n",
    "        query = QUERY_TEMPLATE.format(limit=rows_per_chunk, offset=offset)\n",
    "        query_job = client.query(query)\n",
    "        rows = query_job.result()\n",
    "        df = rows.to_dataframe()\n",
    "        df[\"title_with_body\"] = df.title + \"\\n\" + df.body\n",
    "        yield df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b43937b6065d"
   },
   "outputs": [],
   "source": [
    "# Get a dataframe of 1000 rows for demonstration purposes\n",
    "df = next(query_bigquery_chunks(max_rows=1000, rows_per_chunk=1000))\n",
    "\n",
    "# Examine the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1124422cc200"
   },
   "source": [
    "### Instantiate the text encoding model\n",
    "\n",
    "Use the [Vertex AI Embeddings for Text API](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings) developed by Google for converting text to embeddings.\n",
    "\n",
    "> Text embeddings are a dense vector representation of a piece of content such that, if two pieces of content are semantically similar, their respective embeddings are located near each other in the embedding vector space. This representation can be used to solve common NLP tasks, such as:\n",
    "> - Semantic search: Search text ranked by semantic similarity.\n",
    "> - Recommendation: Return items with text attributes similar to the given text.\n",
    "> - Classification: Return the class of items whose text attributes are similar to the given text.\n",
    "> - Clustering: Cluster items whose text attributes are similar to the given text.\n",
    "> - Outlier Detection: Return items where text attributes are least related to the given text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43088937e820"
   },
   "source": [
    "### Defining an encoding function\n",
    "\n",
    "Define a function to be used later that will take sentences and convert them to embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5c4520ae99f8"
   },
   "outputs": [],
   "source": [
    "# Load the \"Vertex AI Embeddings for Text\" model\n",
    "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
    "\n",
    "# Define an embedding method that uses the model\n",
    "def encode_texts_to_embeddings(sentences: List[str]) -> List[Optional[List[float]]]:\n",
    "    try:\n",
    "        embeddings = model.get_embeddings(sentences)\n",
    "        return [embedding.values for embedding in embeddings]\n",
    "    except Exception:\n",
    "        return [None for _ in range(len(sentences))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eda80c5936ea"
   },
   "source": [
    "### Define two more helper functions for converting text to embeddings\n",
    "\n",
    "- *generate_batches*: According to the documentation, each request can handle up to 5 text instances. Therefore, this method splits `sentences` into batches of 5 before sending to the embedding API.\n",
    "- *encode_text_to_embedding_batched*: This method calls `generate_batches` to handle batching and then calls the embedding API via `encode_texts_to_embeddings`. It also handles rate-limiting using `time.sleep`. For production use cases, you would want a more sophisticated rate-limiting mechanism that takes retries into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0370bd840d2"
   },
   "outputs": [],
   "source": [
    "# Generator function to yield batches of sentences\n",
    "def generate_batches(\n",
    "    sentences: List[str], batch_size: int\n",
    ") -> Generator[List[str], None, None]:\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        yield sentences[i : i + batch_size]\n",
    "\n",
    "\n",
    "def encode_text_to_embedding_batched(\n",
    "    sentences: List[str], api_calls_per_second: int = 10, batch_size: int = 5\n",
    ") -> Tuple[List[bool], np.ndarray]:\n",
    "\n",
    "    embeddings_list: List[List[float]] = []\n",
    "\n",
    "    # Prepare the batches using a generator\n",
    "    batches = generate_batches(sentences, batch_size)\n",
    "\n",
    "    seconds_per_job = 1 / api_calls_per_second\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for batch in tqdm(\n",
    "            batches, total=math.ceil(len(sentences) / batch_size), position=0\n",
    "        ):\n",
    "            futures.append(\n",
    "                executor.submit(functools.partial(encode_texts_to_embeddings), batch)\n",
    "            )\n",
    "            time.sleep(seconds_per_job)\n",
    "\n",
    "        for future in futures:\n",
    "            embeddings_list.extend(future.result())\n",
    "\n",
    "    is_successful = [\n",
    "        embedding is not None for sentence, embedding in zip(sentences, embeddings_list)\n",
    "    ]\n",
    "    embeddings_list_successful = np.squeeze(\n",
    "        np.stack([embedding for embedding in embeddings_list if embedding is not None])\n",
    "    )\n",
    "    return is_successful, embeddings_list_successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba45d58bf96e"
   },
   "source": [
    "### Test the encoding function\n",
    "\n",
    "Encode a subset of data and see if the embeddings and distance metrics make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9b01baa906b5"
   },
   "outputs": [],
   "source": [
    "# Encode a subset of questions for validation\n",
    "questions = df.title.tolist()[:500]\n",
    "is_successful, question_embeddings = encode_text_to_embedding_batched(\n",
    "    sentences=df.title.tolist()[:500]\n",
    ")\n",
    "\n",
    "# Filter for successfully embedded sentences\n",
    "questions = np.array(questions)[is_successful]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3761f56648b"
   },
   "source": [
    "Save the dimension size for later usage when creating the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d296e181205d"
   },
   "outputs": [],
   "source": [
    "DIMENSIONS = len(question_embeddings[0])\n",
    "\n",
    "print(DIMENSIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d503db448252"
   },
   "source": [
    "#### Sort questions in order of similarity\n",
    "\n",
    "Similarity of two embeddings can be calculated using the dot-product between them. \n",
    "\n",
    "In this step, you perform the following tasks:\n",
    "\n",
    "- Calculate vector similarity using `np.dot`\n",
    "- Sort by similarity scores\n",
    "- Print results for inspection\n",
    "\n",
    "Learn more about the [embeddings and semantic search](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings#colab_example_of_semantic_search_using_embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95e408daf219"
   },
   "outputs": [],
   "source": [
    "question_index = random.randint(0, 99)\n",
    "\n",
    "print(f\"Query question = {questions[question_index]}\")\n",
    "\n",
    "# Get similarity scores for each embedding by using dot-product.\n",
    "scores = np.dot(question_embeddings[question_index], question_embeddings.T)\n",
    "\n",
    "# Print top 20 matches\n",
    "for index, (question, score) in enumerate(\n",
    "    sorted(zip(questions, scores), key=lambda x: x[1], reverse=True)[:20]\n",
    "):\n",
    "    print(f\"\\t{index}: {question}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQIQSyF9GtSv"
   },
   "source": [
    "### Save the embeddings in JSONL format\n",
    "\n",
    "The data must be formatted in JSONL format, which means each embedding dictionary is written as an individual JSON object on its own line.\n",
    "\n",
    "See more information about [input data format and structure](https://cloud.google.com/vertex-ai/docs/matching-engine/match-eng-setup/format-structure#data-file-formats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7c1193aca5d1"
   },
   "outputs": [],
   "source": [
    "# Create temporary file to write embeddings to\n",
    "embeddings_file_path = Path(tempfile.mkdtemp())\n",
    "\n",
    "print(f\"Embeddings directory: {embeddings_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "279c0bbfc6bb"
   },
   "source": [
    "Write embeddings in batches to prevent out-of-memory errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "307f468a3ecd"
   },
   "outputs": [],
   "source": [
    "# Set the number of rows needed from the dataset\n",
    "BQ_NUM_ROWS = 5000\n",
    "# Set the chunk size for querying and embedding generation\n",
    "BQ_CHUNK_SIZE = 1000\n",
    "# Calculate the number of chunks\n",
    "BQ_NUM_CHUNKS = math.ceil(BQ_NUM_ROWS / BQ_CHUNK_SIZE)\n",
    "# Initialize the starting chunk index\n",
    "START_CHUNK = 0\n",
    "\n",
    "# Create a rate limit of 300 requests per minute. Adjust this depending on your quota.\n",
    "API_CALLS_PER_SECOND = 300 / 60\n",
    "# According to the docs, each request can process 5 instances per request\n",
    "ITEMS_PER_REQUEST = 5\n",
    "\n",
    "# Loop through each generated dataframe, convert\n",
    "for i, df in tqdm(\n",
    "    enumerate(\n",
    "        query_bigquery_chunks(\n",
    "            max_rows=BQ_NUM_ROWS, rows_per_chunk=BQ_CHUNK_SIZE, start_chunk=START_CHUNK\n",
    "        )\n",
    "    ),\n",
    "    total=BQ_NUM_CHUNKS - START_CHUNK,\n",
    "    position=-1,\n",
    "    desc=\"Chunk of rows from BigQuery\",\n",
    "):\n",
    "    # Create a unique output file for each chunk\n",
    "    chunk_path = embeddings_file_path.joinpath(\n",
    "        f\"{embeddings_file_path.stem}_{i+START_CHUNK}.json\"\n",
    "    )\n",
    "    with open(chunk_path, \"a\") as f:\n",
    "        id_chunk = df.id\n",
    "\n",
    "        # Convert batch to embeddings\n",
    "        is_successful, question_chunk_embeddings = encode_text_to_embedding_batched(\n",
    "            sentences=df.title_with_body,\n",
    "            api_calls_per_second=API_CALLS_PER_SECOND,\n",
    "            batch_size=ITEMS_PER_REQUEST,\n",
    "        )\n",
    "\n",
    "        # Append to file\n",
    "        embeddings_formatted = [\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"id\": str(id),\n",
    "                    \"embedding\": [str(value) for value in embedding],\n",
    "                }\n",
    "            )\n",
    "            + \"\\n\"\n",
    "            for id, embedding in zip(id_chunk[is_successful], question_chunk_embeddings)\n",
    "        ]\n",
    "        f.writelines(embeddings_formatted)\n",
    "\n",
    "        # Delete the DataFrame and any other large data structures\n",
    "        del df\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuVl8DrWG8NS"
   },
   "source": [
    "Upload the training data to a Google Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PgsA_vbI8Vg"
   },
   "outputs": [],
   "source": [
    "remote_folder = f\"{BUCKET_URI}/{embeddings_file_path.stem}/\"\n",
    "! gsutil -m cp -r {embeddings_file_path}/* {remote_folder}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mglUPwHpJH98"
   },
   "source": [
    "## Create an Index\n",
    "\n",
    "Configure and create an Index resource in Matching Engine which uses the ANN service (for Production Usage).\n",
    "\n",
    "Learn more about [ANN service](https://cloud.google.com/vertex-ai/docs/matching-engine/ann-service-overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dffb00b23f5a"
   },
   "outputs": [],
   "source": [
    "# Set display name for your index\n",
    "INDEX_DISPLAY_NAME = \"stack_overflow_index_unique\" # @param {type:\"string\"}\n",
    "# Set description for your index\n",
    "INDEX_DESCRIPTION = \"Query titles and bodies from stackoverflow.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Matching Engine Index can be created using two methods:\n",
    "1. `create_tree_ah_index`: Creates an Index resource that uses the asymmetric hashing(AH) algorithm.\n",
    "2. `create_brute_force_index`: Creates an Index resource that uses the brute force algorithm.\n",
    "\n",
    "Learn more about the [brute force and AH methods](https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md).\n",
    "\n",
    "For a faster perfomance requirement as in production usecases, the AH method is recommended. \n",
    "\n",
    "Specify the below parameters to create a Matching Engine Index:\n",
    "\n",
    "- `display_name`:  The display name of the Index.\n",
    "\n",
    "- `contents_delta_uri`: Allows inserting, updating  or deleting the contents of the Matching Engine Index. The string must be a valid Google Cloud Storage directory path. \n",
    "\n",
    "- `dimensions`: The number of dimensions of the input vectors.\n",
    "\n",
    "- `approximate_neighbors_count`: The default number of neighbors to find through approximate search before exact reordering is performed. Exact reordering is a procedure where results returned by an approximate search algorithm are reordered via a more expensive distance computation.\n",
    "\n",
    "- `distance_measure_type`: The distance measure used in nearest neighbor search.\n",
    "\n",
    "- `leaf_node_embedding_count`: Number of embeddings on each leaf node. The default value is 1000 if not set.\n",
    "\n",
    "- `leaf_nodes_to_search_percent`: The default percentage of leaf nodes that any query may be searched. Must be in range 1-100, inclusive. The default value is 10 (means 10%) if not set.\n",
    "\n",
    "- `description`: The description of the Index.  \n",
    "\n",
    "Learn more about the [parameters to configure Indexes](https://cloud.google.com/vertex-ai/docs/matching-engine/configuring-indexes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the index\n",
    "tree_ah_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "    display_name=INDEX_DISPLAY_NAME,\n",
    "    contents_delta_uri=remote_folder,\n",
    "    dimensions=DIMENSIONS,\n",
    "    approximate_neighbors_count=150,\n",
    "    distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
    "    leaf_node_embedding_count=500,\n",
    "    leaf_nodes_to_search_percent=80,\n",
    "    description=INDEX_DESCRIPTION,\n",
    ")\n",
    "# obtain the Index resource name\n",
    "INDEX_RESOURCE_NAME = tree_ah_index.resource_name\n",
    "INDEX_RESOURCE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f1a9fbecabb"
   },
   "source": [
    "Using the resource name, you can retrieve an existing MatchingEngineIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ddb70647d98"
   },
   "outputs": [],
   "source": [
    "tree_ah_index = aiplatform.MatchingEngineIndex(index_name=INDEX_RESOURCE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qV2xjAnDDObD"
   },
   "source": [
    "## Create an IndexEndpoint\n",
    "\n",
    "To query and use your created Index, you need to deploy your Index to an IndexEndpoint. This concept is similar to how Vertex AI Models need to be deployed to an Endpoint for online predictions.\n",
    "\n",
    "Sepcify the following parameter sto create an IndexEndpoint:\n",
    "\n",
    "- `display_name`: The display name of the IndexEndpoint.\n",
    "- `description`: The description of the IndexEndpoint.\n",
    "- `public_endpoint_enabled`: If true, the deployed index will be accessible through public endpoint.\n",
    "\n",
    "Learn more about [creating IndexEndpoints](https://cloud.google.com/vertex-ai/docs/matching-engine/deploy-index-public#create-index-endpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuARXzJVGyQX"
   },
   "outputs": [],
   "source": [
    "# Set the display name\n",
    "ENDPOINT_DISPLAY_NAME = \"stack_overflow_endpoint_unique\" # @param {type:\"string\"}\n",
    "ENDPOINT_DESCRIPTION = \"IndexEndpoint to receive the query requests for stackoverflow.\"\n",
    "\n",
    "my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    description=DISPLAY_NAME,\n",
    "    public_endpoint_enabled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "np2cgVuuIe9k"
   },
   "source": [
    "## Deploy the Index\n",
    "\n",
    "Set an id for your deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLOYTGygIlMK"
   },
   "outputs": [],
   "source": [
    "# Set deployment id\n",
    "DEPLOYED_INDEX_ID = \"stack_overflow_deployment_unique\" # @param {type:\"string\"}\n",
    "\n",
    "DEPLOYED_INDEX_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy your Index to the IndexEndpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_uK4WOgqN1NG"
   },
   "outputs": [],
   "source": [
    "# Deploy the Index\n",
    "my_index_endpoint = my_index_endpoint.deploy_index(\n",
    "    index=tree_ah_index, \n",
    "    deployed_index_id=DEPLOYED_INDEX_ID\n",
    ")\n",
    "\n",
    "my_index_endpoint.deployed_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cbfe4fd103a"
   },
   "source": [
    "### Verify number of declared items matches the number of embeddings\n",
    "\n",
    "Each IndexEndpoint can have multiple indexes deployed to it. For each index, you can retrieved the number of deployed vectors using the `index_endpoint._gca_resource.index_stats.vectors_count`. The numbers may not match exactly due to potential failures using the embedding service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93f89a15f642"
   },
   "outputs": [],
   "source": [
    "number_of_vectors = sum(\n",
    "    aiplatform.MatchingEngineIndex(\n",
    "        deployed_index.index\n",
    "    )._gca_resource.index_stats.vectors_count\n",
    "    for deployed_index in my_index_endpoint.deployed_indexes\n",
    ")\n",
    "\n",
    "print(f\"Expected: {BQ_NUM_ROWS}, Actual: {number_of_vectors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LCGvBNvBd8D"
   },
   "source": [
    "## Create online queries\n",
    "\n",
    "Specify your query and encode it as an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ae9996f185fe"
   },
   "outputs": [],
   "source": [
    "# Specify your query(s)\n",
    "QUERY_LIST = [\"Install GPU for Tensorflow\"]\n",
    "# Encode your query\n",
    "test_embeddings = encode_texts_to_embeddings(sentences=QUERY_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you build your Index, you may query against the deployed Index to find nearest neighbors.\n",
    "\n",
    "Note: For the **DOT_PRODUCT_DISTANCE** distance type, the \"distance\" property returned with each MatchNeighbor object actually refers to the similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3KYVw5HB-4v"
   },
   "outputs": [],
   "source": [
    "# Set number of neighbors to return\n",
    "NUM_NEIGHBOURS = 10\n",
    "# Query the deployed index\n",
    "response = my_index_endpoint.find_neighbors(\n",
    "    deployed_index_id=DEPLOYED_INDEX_ID,\n",
    "    queries=test_embeddings,\n",
    "    num_neighbors=NUM_NEIGHBOURS,\n",
    ")\n",
    "# show response\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8a2879d3d9ca"
   },
   "source": [
    "Verify that the retrieved results are relevant by checking the StackOverflow links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7c8682079e21"
   },
   "outputs": [],
   "source": [
    "for match_index, neighbor in enumerate(response[0]):\n",
    "    print(f\"https://stackoverflow.com/questions/{neighbor.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05514825ba7d"
   },
   "source": [
    "## Storing and retrieving titles from a Redis data store\n",
    "\n",
    "When you productionize this code into a service, you need to convert the nearest IDs returned from Vertex AI Matching Engine into data ready to be used by downstream services.\n",
    "\n",
    "In this case, you need to convert the IDs back to titles. You can use Google Cloud's Memorystore to deploy a managed Redis instance to save the id-title key-value pairs.\n",
    "\n",
    "To learn more, see [Create and manage Redis instances](https://cloud.google.com/memorystore/docs/redis/create-manage-instances?hl=en).\n",
    "\n",
    "**Note:** Below, you create a Redis instance with the default `DIRECT_PEERING` mode. Learn more about the [modes available for Redis instances](https://cloud.google.com/memorystore/docs/redis/networking?hl=en#connection_modes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5d2b240f0d52"
   },
   "outputs": [],
   "source": [
    "# Set the Redis instance name\n",
    "REDIS_INSTANCE_NAME = \"stackoverflow-questions-unique\" # @param {type:\"string\"}\n",
    "\n",
    "# Create a Redis instance\n",
    "! gcloud redis instances create '{REDIS_INSTANCE_NAME}' --size=10 --region='{REGION}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the host address and port information for the created Redis instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "371ccc0d2eb2"
   },
   "outputs": [],
   "source": [
    "# Get host and port info\n",
    "REDIS_HOST = ! gcloud redis instances list --filter=\"INSTANCE_NAME:'{REDIS_INSTANCE_NAME}'\" --region {REGION}  --format='value(HOST)'\n",
    "REDIS_PORT = ! gcloud redis instances list --filter=\"INSTANCE_NAME:'{REDIS_INSTANCE_NAME}'\" --region {REGION} --format='value(PORT)'\n",
    "\n",
    "if isinstance(REDIS_HOST, list):\n",
    "    REDIS_HOST = REDIS_HOST[0]\n",
    "\n",
    "if isinstance(REDIS_PORT, list):\n",
    "    REDIS_PORT = REDIS_PORT[0]\n",
    "\n",
    "print(f\"REDIS_HOST = {REDIS_HOST}\")\n",
    "print(f\"REDIS_PORT = {REDIS_PORT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the Redis instance as a client using the Redis SDK for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73796089386a"
   },
   "outputs": [],
   "source": [
    "# Connect to the instance\n",
    "redis_client = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Redis pipeline and set the specified fields (IDs) to their respective values(Title, Body) in the hash stored at key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f000f5432d13"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Convert the id -> (title, body) relationship into a dict and write to Redis\n",
    "for df in tqdm(\n",
    "    query_bigquery_chunks(\n",
    "        max_rows=BQ_NUM_ROWS, rows_per_chunk=BQ_CHUNK_SIZE, start_chunk=0\n",
    "    ),\n",
    "    total=BQ_NUM_CHUNKS,\n",
    "    position=0,\n",
    "    desc=\"Chunk of rows from BigQuery\",\n",
    "):\n",
    "    ids = df.id.tolist()\n",
    "    titles = df.title.tolist()\n",
    "    bodies = df.body.tolist()\n",
    "\n",
    "    # create a Redis pipeline\n",
    "    pipe = redis_client.pipeline()\n",
    "\n",
    "    # iterate over the data and add hset commands to the pipeline\n",
    "    for (id, title, body) in tqdm(zip(ids, titles, bodies), total=len(ids), position=1):\n",
    "        pipe.hset(\n",
    "            str(id),\n",
    "            mapping={\n",
    "                \"title\": str(title),\n",
    "                \"body\": str(body[:100]),\n",
    "            },\n",
    "        )\n",
    "\n",
    "    # execute the pipeline\n",
    "    _ = pipe.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select 10 IDs at random from the dataset for querying and verify the responses from the Redis instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1f8b396aeb1"
   },
   "outputs": [],
   "source": [
    "# Verify that Redis can retrieve the correct information\n",
    "df = df.sample(10)\n",
    "\n",
    "[\n",
    "    f\"Actual = {title}, Retrieved = {redis_client.hgetall(str(id))}\"\n",
    "    for id, title in zip(df.id, df.title)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "- IndexEndpoint\n",
    "- Index\n",
    "- Redis instance\n",
    "- Cloud Storage bucket (set `delete_bucket` to **True** for deletion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "# Force undeployment of indexes and delete endpoint\n",
    "my_index_endpoint.delete(force=True)\n",
    "\n",
    "# Delete indexes\n",
    "tree_ah_index.delete()\n",
    "\n",
    "# Delete redis instance\n",
    "! gcloud redis instances delete '{REDIS_INSTANCE_NAME}' --region {REGION} --quiet\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "delete_bucket = False\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sdk_matching_engine_create_stack_overflow_embeddings_vertex.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "test_kernel",
   "name": "common-cpu.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m109"
  },
  "kernelspec": {
   "display_name": "test_kernel",
   "language": "python",
   "name": "test_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
