{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Using Vertex AI Vector Search for StackOverflow Questions\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/vector_search/sdk_vector_search_create_stack_overflow_embeddings.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fvector_search%2Fsdk_vector_search_create_stack_overflow_embeddings.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>    \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/vector_search/sdk_vector_search_create_stack_overflow_embeddings.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/vector_search/sdk_vector_search_create_stack_overflow_embeddings.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0a74aaf1481"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This example demonstrates how to encode custom text embeddings using the StackOverflow dataset and the sentence-T5 model. These are uploaded to the Vertex AI Vector Search service. It's a high scale, low latency solution, to find similar vectors (or more specifically \"embeddings\") for a large corpus. Moreover, it's a fully managed offering, further reducing operational overhead. The Vertex AI Vector Search service is built upon [Approximate Nearest Neighbor (ANN) technology](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html) developed by Google Research.\n",
        "\n",
        "**Pre-requisite**: This notebook requires you to already have a VPC network set up. See the \"Prepare a VPC network\" section in [Create Vertex AI Vector Search index notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/vector_search/sdk_vector_search_for_indexing.ipynb).\n",
        "\n",
        "Learn more about [Vertex AI Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34a4b245e795"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this notebook, you learn how to encode custom text embeddings, create an Approximate Nearest Neighbor (ANN) index, and query against indexes.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- Vertex AI Vector Search\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "* Create ANN index.\n",
        "* Create an index endpoint with VPC Network.\n",
        "* Deploy ANN index.\n",
        "* Perform online query.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used for this tutorial is the [StackOverflow dataset](https://console.cloud.google.com/marketplace/product/stack-exchange/stack-overflow).\n",
        "\n",
        "> Stack Overflow is the largest online community for programmers to learn, share their knowledge, and advance their careers. Updated on a quarterly basis, this BigQuery dataset includes an archive of Stack Overflow content, including posts, votes, tags, and badges. This dataset is updated to mirror the Stack Overflow content on the Internet Archive, and is also available through the Stack Exchange Data Explorer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0be1c1c229a"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0f1bea346db"
      },
      "source": [
        "### Install Vertex AI SDK for Python and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dfbccc635a17"
      },
      "outputs": [],
      "source": [
        "# Install the google-cloud packages\n",
        "! pip3 install --upgrade google-cloud-aiplatform \\\n",
        "                         google-cloud-storage \\\n",
        "                         'google-cloud-bigquery[pandas]' -q\n",
        "\n",
        "# Install the latest version of tensorflow packages\n",
        "! pip3 install --upgrade tensorflow \\\n",
        "                         tensorflow_text \\\n",
        "                         tensorflow-hub -q\n",
        "\n",
        "# Install the redis and tqdm packages\n",
        "! pip install --upgrade redis \\\n",
        "                        tqdm -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b08ba354c6e"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages, you must restart the runtime on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bea801acf6b5"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffcde4d56c00"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7176ea64999b"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "Authenticate your environment on Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7de6ef0fac42"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd28c9e4f067"
      },
      "source": [
        "### Set Google Cloud project information\n",
        "\n",
        "Learn more about how to [set up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "80c0215f05a0"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"vertexai-service-project\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = f\"gs://k-bucket-{PROJECT_ID}-vector-s-sf\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcIXiGsCePi"
      },
      "source": [
        "**If your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating gs://k-bucket-vertexai-service-project-vector-s-sf/...\n"
          ]
        }
      ],
      "source": [
        "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e45533d84985"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "b519c49740cb"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44b8dbd3f108"
      },
      "source": [
        "### Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bf6f53cd87ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-06-17 12:45:31.556667: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-17 12:45:32.917325: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import tempfile\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import redis\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "# Registers the ops.\n",
        "import tensorflow_text as text  # noqa: F401\n",
        "from google.cloud import bigquery\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR6Wwv-hCCN-"
      },
      "source": [
        "## Prepare the data\n",
        "\n",
        "For this tutorial, use the [Stack Overflow dataset](https://console.cloud.google.com/marketplace/product/stack-exchange/stack-overflow) of question and answers hosted on BigQuery.\n",
        "\n",
        "> This public dataset is hosted in Google BigQuery and is included in BigQuery's 1TB/mo of free tier processing. Each user receives 1TB of free BigQuery processing every month, which can be used to run queries on this public dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62f6feea28a2"
      },
      "source": [
        "Fetch the dataset from the BigQuery source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9wzS85TeB9dG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 49.2 ms, sys: 473 μs, total: 49.7 ms\n",
            "Wall time: 12.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "NUM_ROWS = 1000\n",
        "NUM_ROWS = 10  ##TODO: Remove Line\n",
        "\n",
        "QUERY = f\"\"\"\n",
        "        SELECT distinct q.id, q.title, q.body, q.tags, a.body as answers, a.score \n",
        "        FROM (SELECT * FROM `bigquery-public-data.stackoverflow.posts_questions` where Score>0 ORDER BY View_Count desc) AS q \n",
        "        INNER JOIN (SELECT * FROM `bigquery-public-data.stackoverflow.posts_answers`  where Score>0 ORDER BY Score desc) AS a ON q.id = a.parent_id \n",
        "        where q.tags like '%python%'\n",
        "        LIMIT {NUM_ROWS};\n",
        "        \"\"\"\n",
        "\n",
        "query_job = client.query(QUERY)\n",
        "rows = query_job.result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "b43937b6065d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "      <th>tags</th>\n",
              "      <th>answers</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30757272</td>\n",
              "      <td>Pivot each group in Pandas</td>\n",
              "      <td>&lt;p&gt;Using Pandas I've invoked groupby on my dat...</td>\n",
              "      <td>python|pandas|pivot</td>\n",
              "      <td>&lt;p&gt;Assuming you have a frame looking like&lt;/p&gt;\\...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>63822959</td>\n",
              "      <td>Any workaround to make moving average time ser...</td>\n",
              "      <td>&lt;p&gt;I want to understand how covid pandemic is ...</td>\n",
              "      <td>python|matplotlib|data-visualization</td>\n",
              "      <td>&lt;p&gt;We have graphed the moving average of the n...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>63947820</td>\n",
              "      <td>Pandas DataFrame filter rows using another Dat...</td>\n",
              "      <td>&lt;pre&gt;&lt;code&gt;import pandas as pd\\nimport numpy a...</td>\n",
              "      <td>python|pandas|dataframe</td>\n",
              "      <td>&lt;p&gt;My method would be similar to @Ben_Yo 's me...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60296637</td>\n",
              "      <td>How are neural networks, loss and optimizer co...</td>\n",
              "      <td>&lt;p&gt;I've seen answers to &lt;a href=\"https://stack...</td>\n",
              "      <td>python|neural-network|pytorch|gradient-descent...</td>\n",
              "      <td>&lt;blockquote&gt;\\n  &lt;p&gt;Optimizer is initialized wi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>40222181</td>\n",
              "      <td>Pandas dataframe - create new column based on ...</td>\n",
              "      <td>&lt;p&gt;I want to make a calculation based on 4 col...</td>\n",
              "      <td>python|pandas|dataframe</td>\n",
              "      <td>&lt;p&gt;you can use &lt;a href=\"https://docs.scipy.org...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                                              title  \\\n",
              "0  30757272                         Pivot each group in Pandas   \n",
              "1  63822959  Any workaround to make moving average time ser...   \n",
              "2  63947820  Pandas DataFrame filter rows using another Dat...   \n",
              "3  60296637  How are neural networks, loss and optimizer co...   \n",
              "4  40222181  Pandas dataframe - create new column based on ...   \n",
              "\n",
              "                                                body  \\\n",
              "0  <p>Using Pandas I've invoked groupby on my dat...   \n",
              "1  <p>I want to understand how covid pandemic is ...   \n",
              "2  <pre><code>import pandas as pd\\nimport numpy a...   \n",
              "3  <p>I've seen answers to <a href=\"https://stack...   \n",
              "4  <p>I want to make a calculation based on 4 col...   \n",
              "\n",
              "                                                tags  \\\n",
              "0                                python|pandas|pivot   \n",
              "1               python|matplotlib|data-visualization   \n",
              "2                            python|pandas|dataframe   \n",
              "3  python|neural-network|pytorch|gradient-descent...   \n",
              "4                            python|pandas|dataframe   \n",
              "\n",
              "                                             answers  score  \n",
              "0  <p>Assuming you have a frame looking like</p>\\...      5  \n",
              "1  <p>We have graphed the moving average of the n...      2  \n",
              "2  <p>My method would be similar to @Ben_Yo 's me...      3  \n",
              "3  <blockquote>\\n  <p>Optimizer is initialized wi...      1  \n",
              "4  <p>you can use <a href=\"https://docs.scipy.org...      2  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert to a dataframe\n",
        "df = rows.to_dataframe()\n",
        "\n",
        "# Examine the data\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2cacd9869ee5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Extract the question ids and question text\n",
        "ids = df.id.tolist()\n",
        "questions = df.title.tolist()\n",
        "\n",
        "# Verify the length\n",
        "len(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1124422cc200"
      },
      "source": [
        "### Instantiate the text encoding model\n",
        "\n",
        "Use the [sentence-t5 encoder](https://tfhub.dev/google/sentence-t5/st5-base/1) developed by Google for converting text to embeddings.\n",
        "\n",
        "> The sentence-T5 family of models encode text into high-dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language processing tasks.\n",
        ">\n",
        "> The model is built on top of T5 (i.e., the Text-To-Text Transfer Transformer). It's trained on a variety of data sources and initialized from pre-trained T5 models with different model sizes. The input is variable-length English text and the output is a 768-dimensional vector. The sentence-T5 base model employs a 12-layer transformer architecture as does the T5 base model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ed41c7712930"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Importing a function (__inference_<lambda>_9720) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_<lambda>_3354) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_<lambda>_6722) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
          ]
        }
      ],
      "source": [
        "hub_url = \"https://tfhub.dev/google/sentence-t5/st5-base/1\"\n",
        "\n",
        "encoder = hub.KerasLayer(hub_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43088937e820"
      },
      "source": [
        "### Define an encoding function\n",
        "\n",
        "Define a function, to be used later, that takes sentences and converts them to embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "a0370bd840d2"
      },
      "outputs": [],
      "source": [
        "def encode_text_to_embedding(\n",
        "    text_encoder: hub.KerasLayer, sentences: List[str], batch_size: int = 100\n",
        ") -> np.ndarray:\n",
        "    embeddings_list = []\n",
        "\n",
        "    # Process data in chunks to prevent out-of-memory errors\n",
        "    for i in tqdm(range(0, len(sentences), batch_size)):\n",
        "        batch = sentences[i : i + batch_size]\n",
        "        embeddings_list.append(text_encoder(tf.constant(batch)))\n",
        "\n",
        "    return np.squeeze(np.column_stack(embeddings_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba45d58bf96e"
      },
      "source": [
        "#### Test the encoding function\n",
        "\n",
        "Encode a subset of data and see if the embeddings and distance metrics make sense.\n",
        "\n",
        "According to the [sentence-T5 research paper](https://arxiv.org/pdf/2108.08877.pdf), the similarity of embeddings is calculated using the dot-product. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9b01baa906b5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "404a5428ced54f779ebc37d3ef142d82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Encode 500 questions\n",
        "questions = df.title.tolist()[:500]\n",
        "questions = df.title.tolist()[:5]  ##TODO: Remove Line\n",
        "question_embeddings = encode_text_to_embedding(\n",
        "    text_encoder=encoder, sentences=questions\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3761f56648b"
      },
      "source": [
        "Save the dimension size for later usage when creating the index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "d296e181205d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "768\n"
          ]
        }
      ],
      "source": [
        "DIMENSIONS = len(question_embeddings[0])\n",
        "\n",
        "print(DIMENSIONS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "95e408daf219"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query question = Pivot each group in Pandas\n",
            "\t0: Pivot each group in Pandas: 0.9999998211860657\n",
            "\t1: Pandas DataFrame filter rows using another DataFrame Column: 0.8553687334060669\n",
            "\t2: Pandas dataframe - create new column based on simple calcuation: 0.8426516056060791\n",
            "\t3: Any workaround to make moving average time series line plot in matplotlib?: 0.7910910844802856\n",
            "\t4: How are neural networks, loss and optimizer connected in PyTorch?: 0.7272672057151794\n"
          ]
        }
      ],
      "source": [
        "question_index = 0\n",
        "\n",
        "# Print the query question\n",
        "print(f\"Query question = {questions[question_index]}\")\n",
        "scores = np.dot(question_embeddings[question_index], question_embeddings.T)\n",
        "\n",
        "# Print top 20 matches\n",
        "for index, (question, score) in enumerate(\n",
        "    sorted(zip(questions, scores), key=lambda x: x[1], reverse=True)[:20]\n",
        "):\n",
        "    print(f\"\\t{index}: {question}: {score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQIQSyF9GtSv"
      },
      "source": [
        "### Save the train split in JSONL format.\n",
        "\n",
        "The data must be formatted in JSONL format, which means each embedding dictionary is written as a JSON string on its own line.\n",
        "\n",
        "See more information in the docs for [input data format and structure](https://cloud.google.com/vertex-ai/docs/vector-search/setup/format-structure#data-file-formats)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7c1193aca5d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/var/tmp/tmp3bf38e59.json\n"
          ]
        }
      ],
      "source": [
        "# Create temporary file to write embeddings to\n",
        "embeddings_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=False)\n",
        "\n",
        "print(embeddings_file.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "307f468a3ecd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "116a5073a7294bb29224dc77007ede4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f0143a9beb643c2a35ebd5d2eadaa7f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Set batch size\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "# Create embeddings and write to a file\n",
        "with open(embeddings_file.name, \"a\") as f:\n",
        "    for i in tqdm(range(0, len(questions), BATCH_SIZE)):\n",
        "        id_chunk = ids[i : i + BATCH_SIZE]\n",
        "\n",
        "        question_chunk_embeddings = encode_text_to_embedding(\n",
        "            text_encoder=encoder, sentences=questions[i : i + BATCH_SIZE]\n",
        "        )\n",
        "\n",
        "        # Append to file\n",
        "        embeddings_formatted = [\n",
        "            json.dumps(\n",
        "                {\n",
        "                    \"id\": str(id),\n",
        "                    \"embedding\": [str(value) for value in embedding],\n",
        "                }\n",
        "            )\n",
        "            + \"\\n\"\n",
        "            for id, embedding in zip(id_chunk, question_chunk_embeddings)\n",
        "        ]\n",
        "        f.writelines(embeddings_formatted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuVl8DrWG8NS"
      },
      "source": [
        "Upload the training data to a Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3PgsA_vbI8Vg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying file:///var/tmp/tmp3bf38e59.json [Content-Type=application/json]...\n",
            "/ [1 files][ 57.8 KiB/ 57.8 KiB]                                                \n",
            "Operation completed over 1 objects/57.8 KiB.                                     \n"
          ]
        }
      ],
      "source": [
        "UNIQUE_FOLDER_NAME = \"embeddings_folder_unique\"\n",
        "remote_folder = f\"{BUCKET_URI}/{UNIQUE_FOLDER_NAME}/\"\n",
        "! gsutil cp {embeddings_file.name} {remote_folder}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mglUPwHpJH98"
      },
      "source": [
        "## Create Indexes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhIBCQ7dDSbW"
      },
      "source": [
        "### Create ANN Index (for Production Usage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qiIg9b5zJLi1"
      },
      "outputs": [],
      "source": [
        "DISPLAY_NAME = \"stack_overflow\"\n",
        "DESCRIPTION = \"questions from stackoverflow\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svLYiDf0OD2G"
      },
      "source": [
        "Create the ANN index configuration:\n",
        "\n",
        "To learn more about configuring the index, see [Vector Search input data format and structure](https://cloud.google.com/vertex-ai/docs/vector-search/setup/setup).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dffb00b23f5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating MatchingEngineIndex\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index:Creating MatchingEngineIndex\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create MatchingEngineIndex backing LRO: projects/314481442207/locations/us-central1/indexes/6095468164018077696/operations/823135395720986624\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index:Create MatchingEngineIndex backing LRO: projects/314481442207/locations/us-central1/indexes/6095468164018077696/operations/823135395720986624\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MatchingEngineIndex created. Resource name: projects/314481442207/locations/us-central1/indexes/6095468164018077696\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index:MatchingEngineIndex created. Resource name: projects/314481442207/locations/us-central1/indexes/6095468164018077696\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To use this MatchingEngineIndex in another session:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index:To use this MatchingEngineIndex in another session:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "index = aiplatform.MatchingEngineIndex('projects/314481442207/locations/us-central1/indexes/6095468164018077696')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index:index = aiplatform.MatchingEngineIndex('projects/314481442207/locations/us-central1/indexes/6095468164018077696')\n"
          ]
        }
      ],
      "source": [
        "tree_ah_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
        "    display_name=DISPLAY_NAME,\n",
        "    contents_delta_uri=remote_folder,\n",
        "    dimensions=DIMENSIONS,\n",
        "    approximate_neighbors_count=5,  ##TODO: CHAGE FROM 5 to 150\n",
        "    distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
        "    leaf_node_embedding_count=5,  ##TODO: CHAGE FROM 5 to 500\n",
        "    leaf_nodes_to_search_percent=80,\n",
        "    description=DESCRIPTION,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "17jrQi501QyX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "projects/314481442207/locations/us-central1/indexes/6095468164018077696\n"
          ]
        }
      ],
      "source": [
        "INDEX_RESOURCE_NAME = tree_ah_index.resource_name\n",
        "print(INDEX_RESOURCE_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f1a9fbecabb"
      },
      "source": [
        "Using the resource name, you can retrieve an existing MatchingEngineIndex resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1ddb70647d98"
      },
      "outputs": [],
      "source": [
        "tree_ah_index = aiplatform.MatchingEngineIndex(index_name=INDEX_RESOURCE_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f4f0bc64ddb"
      },
      "source": [
        "## Setup VPC peering network\n",
        "\n",
        "To use a Vector Search index, set up a VPC peering network between your project and the Vertex AI Vector Search service project. This eliminates additional hops in network traffic and allows using efficient gRPC protocol.\n",
        "\n",
        "Learn more about [VPC peering](https://cloud.google.com/vertex-ai/docs/general/vpc-peering).\n",
        "\n",
        "**IMPORTANT: you can only setup one VPC peering to servicenetworking.googleapis.com per project.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d85e8f48291a"
      },
      "source": [
        "### Create VPC peering\n",
        "\n",
        "For simplicity, set up VPC peering to the `ucaip-haystack-vpc-network` network. You can create a different network for your project.\n",
        "\n",
        "If you set up VPC peering with any other network, make sure that the network already exists and that your VM is running on that network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "a107544fbabf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created [https://www.googleapis.com/compute/v1/projects/vertexai-service-project/global/addresses/vertex-ai-prediction-peering-range].\n"
          ]
        }
      ],
      "source": [
        "# This is for display only; you can name the range anything.\n",
        "NETWORK = \"ucaip-haystack-vpc-network\"  # @param {type:\"string\"}##TODO: change name\n",
        "PEERING_RANGE_NAME = \"vertex-ai-prediction-peering-range\"\n",
        "# PEERING_RANGE_NAME = \"servicenetworking-googleapis-com\"##TODO: Remove line\n",
        "\n",
        "# NOTE: `prefix-length=16` means a CIDR block with mask /16 is\n",
        "# reserved for use by Google services, such as Vertex AI.\n",
        "! gcloud compute addresses create $PEERING_RANGE_NAME \\\n",
        "  --global \\\n",
        "  --prefix-length=16 \\\n",
        "  --description=\"peering range for Google service\" \\\n",
        "  --network=$NETWORK \\\n",
        "  --purpose=VPC_PEERING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e29cad1a0be"
      },
      "source": [
        "### Create the VPC connection\n",
        "\n",
        "Next, create the connection for VPC peering.\n",
        "\n",
        "**Note:** If you get a PERMISSION DENIED, you may not have the neccessary 'Compute Network Admin' role set for your default service account. In the Cloud Console, do the following:\n",
        "\n",
        "1. Goto **IAM & Admin**.\n",
        "2. Find your service account.\n",
        "3. Click edit icon.\n",
        "4. Select **Add Another Role**.\n",
        "5. Enter **Compute Network Admin**.\n",
        "6. Select **Save**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "f3f6c85ffc63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Operation \"operations/pssn.p24-314481442207-6f158a59-988b-4933-84c0-53b82f36214e\" finished successfully.\n"
          ]
        }
      ],
      "source": [
        "! gcloud services vpc-peerings connect \\\n",
        "  --service=servicenetworking.googleapis.com \\\n",
        "  --network=$NETWORK \\\n",
        "  --ranges=$PEERING_RANGE_NAME \\\n",
        "  --project=$PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "944d772b1397"
      },
      "source": [
        "Check the status of your peering connections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "b946ce37cc16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NAME                              NETWORK                     PEER_PROJECT           PEER_NETWORK       STACK_TYPE  PEER_MTU  IMPORT_CUSTOM_ROUTES  EXPORT_CUSTOM_ROUTES  STATE   STATE_DETAILS\n",
            "servicenetworking-googleapis-com  ucaip-haystack-vpc-network  rdb80d4a362feb3bdp-tp  servicenetworking  IPV4_ONLY   1460      False                 False                 ACTIVE  [2024-06-09T02:12:58.493-07:00]: Connected.\n"
          ]
        }
      ],
      "source": [
        "! gcloud compute networks peerings list --network $NETWORK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a5e1b83ae61"
      },
      "source": [
        "#### Construct the full network name\n",
        "\n",
        "You need to have the full network resource name when you subsequently create an Vector Search index endpoint resource for VPC peering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "BpZQoJyxDlbO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "projects/314481442207/global/networks/ucaip-haystack-vpc-network\n"
          ]
        }
      ],
      "source": [
        "# Retrieve the project number\n",
        "PROJECT_NUMBER = !gcloud projects list --filter=\"PROJECT_ID:'{PROJECT_ID}'\" --format='value(PROJECT_NUMBER)'\n",
        "PROJECT_NUMBER = PROJECT_NUMBER[0]\n",
        "\n",
        "full_network_name = f\"projects/{PROJECT_NUMBER}/global/networks/{NETWORK}\"\n",
        "print(full_network_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV2xjAnDDObD"
      },
      "source": [
        "## Create an IndexEndpoint with VPC Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "QuARXzJVGyQX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating MatchingEngineIndexEndpoint\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint:Creating MatchingEngineIndexEndpoint\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create MatchingEngineIndexEndpoint backing LRO: projects/314481442207/locations/us-central1/indexEndpoints/8291535932314615808/operations/6705188352787742720\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint:Create MatchingEngineIndexEndpoint backing LRO: projects/314481442207/locations/us-central1/indexEndpoints/8291535932314615808/operations/6705188352787742720\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MatchingEngineIndexEndpoint created. Resource name: projects/314481442207/locations/us-central1/indexEndpoints/8291535932314615808\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint:MatchingEngineIndexEndpoint created. Resource name: projects/314481442207/locations/us-central1/indexEndpoints/8291535932314615808\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To use this MatchingEngineIndexEndpoint in another session:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint:To use this MatchingEngineIndexEndpoint in another session:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "index_endpoint = aiplatform.MatchingEngineIndexEndpoint('projects/314481442207/locations/us-central1/indexEndpoints/8291535932314615808')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint:index_endpoint = aiplatform.MatchingEngineIndexEndpoint('projects/314481442207/locations/us-central1/indexEndpoints/8291535932314615808')\n"
          ]
        }
      ],
      "source": [
        "my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
        "    display_name=DISPLAY_NAME,\n",
        "    description=DISPLAY_NAME,\n",
        "    network=full_network_name,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np2cgVuuIe9k"
      },
      "source": [
        "## Deploy Indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ew1UgcIIiJG"
      },
      "source": [
        "### Deploy ANN Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "nLOYTGygIlMK"
      },
      "outputs": [],
      "source": [
        "# Set a unique id for your deployed index\n",
        "DEPLOYED_INDEX_ID = \"deployed_index_id_unique\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "_uK4WOgqN1NG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deploying index MatchingEngineIndexEndpoint index_endpoint: projects/314481442207/locations/us-central1/indexEndpoints/8291535932314615808\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint:Deploying index MatchingEngineIndexEndpoint index_endpoint: projects/314481442207/locations/us-central1/indexEndpoints/8291535932314615808\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deploy index MatchingEngineIndexEndpoint index_endpoint backing LRO: projects/314481442207/locations/us-central1/indexEndpoints/8291535932314615808/operations/5512156663999627264\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint:Deploy index MatchingEngineIndexEndpoint index_endpoint backing LRO: projects/314481442207/locations/us-central1/indexEndpoints/8291535932314615808/operations/5512156663999627264\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_OperationNotComplete\u001b[0m                     Traceback (most recent call last)",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/future/polling.py:120\u001b[0m, in \u001b[0;36mPollingFuture._done_or_raise\u001b[0;34m(self, retry)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone(retry\u001b[38;5;241m=\u001b[39mretry):\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _OperationNotComplete()\n",
            "\u001b[0;31m_OperationNotComplete\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Deploy your ANN index to the index endpoint\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m my_index_endpoint \u001b[38;5;241m=\u001b[39m \u001b[43mmy_index_endpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtree_ah_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeployed_index_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEPLOYED_INDEX_ID\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m my_index_endpoint\u001b[38;5;241m.\u001b[39mdeployed_indexes\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/matching_engine/matching_engine_index_endpoint.py:1137\u001b[0m, in \u001b[0;36mMatchingEngineIndexEndpoint.deploy_index\u001b[0;34m(self, index, deployed_index_id, display_name, machine_type, min_replica_count, max_replica_count, enable_access_logging, reserved_ip_ranges, deployment_group, auth_config_audiences, auth_config_allowed_issuers, request_metadata, deploy_request_timeout)\u001b[0m\n\u001b[1;32m   1126\u001b[0m deploy_lro \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_client\u001b[38;5;241m.\u001b[39mdeploy_index(\n\u001b[1;32m   1127\u001b[0m     index_endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresource_name,\n\u001b[1;32m   1128\u001b[0m     deployed_index\u001b[38;5;241m=\u001b[39mdeployed_index,\n\u001b[1;32m   1129\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mrequest_metadata,\n\u001b[1;32m   1130\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mdeploy_request_timeout,\n\u001b[1;32m   1131\u001b[0m )\n\u001b[1;32m   1133\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_started_against_resource_with_lro(\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeploy index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_endpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, deploy_lro\n\u001b[1;32m   1135\u001b[0m )\n\u001b[0;32m-> 1137\u001b[0m \u001b[43mdeploy_lro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1139\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_completed_against_resource(\n\u001b[1;32m   1140\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_endpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeployed index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1141\u001b[0m )\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;66;03m# update local resource\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/future/polling.py:256\u001b[0m, in \u001b[0;36mPollingFuture.result\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_DEFAULT_VALUE, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, polling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    145\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the result of the operation.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m    This method will poll for operation status periodically, blocking if\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m            the timeout is reached before the operation completes.\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blocking_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolling\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;66;03m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;66;03m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/future/polling.py:137\u001b[0m, in \u001b[0;36mPollingFuture._blocking_poll\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    134\u001b[0m     polling \u001b[38;5;241m=\u001b[39m polling\u001b[38;5;241m.\u001b[39mwith_timeout(timeout)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[43mpolling\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_done_or_raise\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRetryError:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mTimeoutError(\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOperation did not complete within the designated timeout of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolling\u001b[38;5;241m.\u001b[39mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m     )\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:164\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m         _retry_error_helper(\n\u001b[1;32m    154\u001b[0m             exc,\n\u001b[1;32m    155\u001b[0m             deadline,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m             timeout,\n\u001b[1;32m    162\u001b[0m         )\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSleep generator stopped yielding sleep values.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Deploy your ANN index to the index endpoint\n",
        "my_index_endpoint = my_index_endpoint.deploy_index(\n",
        "    index=tree_ah_index, deployed_index_id=DEPLOYED_INDEX_ID\n",
        ")\n",
        "\n",
        "my_index_endpoint.deployed_indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LCGvBNvBd8D"
      },
      "source": [
        "## Create Online Queries\n",
        "\n",
        "After you've built your indexes, you can query against the deployed index to find nearest neighbors.\n",
        "\n",
        "**Note:** For the **DOT_PRODUCT_DISTANCE** distance type, the \"distance\" property returned with each MatchNeighbor actually refers to the similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae9996f185fe"
      },
      "outputs": [],
      "source": [
        "test_embeddings = encode_text_to_embedding(\n",
        "    text_encoder=encoder, sentences=[\"How do I install tensorflow with GPU support?\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3KYVw5HB-4v"
      },
      "outputs": [],
      "source": [
        "# Test query\n",
        "NUM_NEIGHBOURS = 20\n",
        "\n",
        "response = my_index_endpoint.match(\n",
        "    deployed_index_id=DEPLOYED_INDEX_ID,\n",
        "    queries=[test_embeddings.tolist()],\n",
        "    num_neighbors=NUM_NEIGHBOURS,\n",
        ")\n",
        "\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce2cf0297369"
      },
      "source": [
        "Print titles to verify neighbors make sense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c8682079e21"
      },
      "outputs": [],
      "source": [
        "neighbor_ids = [neighbor.id for neighbor in response[0]]\n",
        "neighbor_distances = [neighbor.distance for neighbor in response[0]]\n",
        "\n",
        "for match_index, neighbor in enumerate(response[0]):\n",
        "    titles = df[df.id.astype(str) == neighbor.id].title.tolist()\n",
        "\n",
        "    if len(titles) > 0:\n",
        "        print(\n",
        "            f\"{match_index}: title = '{titles[0]}', distance = {neighbor.distance:0.2f}\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05514825ba7d"
      },
      "source": [
        "## Storing and retrieving titles from a Redis data store\n",
        "When you productionize this code into a service, you need to convert the nearest ids returned from Vertex AI Vector Search into usable data for downstream services.\n",
        "\n",
        "In this case, you need to convert the ids to titles.\n",
        "\n",
        "You can use Google Cloud's Memorystore to deploy a managed Redis instance to save the id-title key-value pairs.\n",
        "\n",
        "See more information on [Memorystore](https://cloud.google.com/memorystore/docs/redis/create-manage-instances?hl=en)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d2b240f0d52"
      },
      "outputs": [],
      "source": [
        "# Set a display name for your Redis instance\n",
        "REDIS_INSTANCE_NAME = \"stackoverflow-questions-unique\"\n",
        "\n",
        "# Create a Redis instance\n",
        "! gcloud redis instances create '{REDIS_INSTANCE_NAME}' --size=5 --region={LOCATION} --network={VPC_NETWORK_FULL} --connect-mode=private-service-access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "371ccc0d2eb2"
      },
      "outputs": [],
      "source": [
        "# Get host and port info\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    REDIS_HOST = ! gcloud redis instances list --filter=\"INSTANCE_NAME:'{REDIS_INSTANCE_NAME}'\" --region {LOCATION}  --format='value(HOST)'\n",
        "    REDIS_PORT = ! gcloud redis instances list --filter=\"INSTANCE_NAME:'{REDIS_INSTANCE_NAME}'\" --region {LOCATION} --format='value(PORT)'\n",
        "\n",
        "    if isinstance(REDIS_HOST, list):\n",
        "        REDIS_HOST = REDIS_HOST[0]\n",
        "\n",
        "    if isinstance(REDIS_PORT, list):\n",
        "        REDIS_PORT = REDIS_PORT[0]\n",
        "\n",
        "    print(f\"REDIS_HOST = {REDIS_HOST}\")\n",
        "    print(f\"REDIS_PORT = {REDIS_PORT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73796089386a"
      },
      "outputs": [],
      "source": [
        "# Connect to the instance\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    redis_client = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f000f5432d13"
      },
      "outputs": [],
      "source": [
        "# Convert the id -> title relationship into a dict and write to redis\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    redis_client.mset({str(id): str(title) for id, title in zip(df.id, df.title)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1f8b396aeb1"
      },
      "outputs": [],
      "source": [
        "# Verify that redis can retrieve the correct information\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    [\n",
        "        f\"Actual = {title}, Retrieved = {redis_client.get(str(id))}\"\n",
        "        for id, title in list(zip(df.id, df.title))[:10]\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "You can also manually delete resources that you created by running the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Undeploying MatchingEngineIndexEndpoint index_endpoint: projects/314481442207/locations/us-central1/indexEndpoints/8291535932314615808\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint:Undeploying MatchingEngineIndexEndpoint index_endpoint: projects/314481442207/locations/us-central1/indexEndpoints/8291535932314615808\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Undeploy MatchingEngineIndexEndpoint index_endpoint backing LRO: projects/314481442207/locations/us-central1/indexEndpoints/8291535932314615808/operations/8651798922974461952\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint:Undeploy MatchingEngineIndexEndpoint index_endpoint backing LRO: projects/314481442207/locations/us-central1/indexEndpoints/8291535932314615808/operations/8651798922974461952\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MatchingEngineIndexEndpoint index_endpoint undeployed. Resource name: projects/314481442207/locations/us-central1/indexEndpoints/8291535932314615808\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint:MatchingEngineIndexEndpoint index_endpoint undeployed. Resource name: projects/314481442207/locations/us-central1/indexEndpoints/8291535932314615808\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleting MatchingEngineIndexEndpoint : projects/314481442207/locations/us-central1/indexEndpoints/8291535932314615808\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.base:Deleting MatchingEngineIndexEndpoint : projects/314481442207/locations/us-central1/indexEndpoints/8291535932314615808\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Delete MatchingEngineIndexEndpoint  backing LRO: projects/314481442207/locations/us-central1/indexEndpoints/8291535932314615808/operations/1576151327166169088\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.base:Delete MatchingEngineIndexEndpoint  backing LRO: projects/314481442207/locations/us-central1/indexEndpoints/8291535932314615808/operations/1576151327166169088\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MatchingEngineIndexEndpoint deleted. . Resource name: projects/314481442207/locations/us-central1/indexEndpoints/8291535932314615808\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.base:MatchingEngineIndexEndpoint deleted. . Resource name: projects/314481442207/locations/us-central1/indexEndpoints/8291535932314615808\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleting MatchingEngineIndex : projects/314481442207/locations/us-central1/indexes/6095468164018077696\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.base:Deleting MatchingEngineIndex : projects/314481442207/locations/us-central1/indexes/6095468164018077696\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Delete MatchingEngineIndex  backing LRO: projects/314481442207/locations/us-central1/indexes/6095468164018077696/operations/293118014574821376\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.base:Delete MatchingEngineIndex  backing LRO: projects/314481442207/locations/us-central1/indexes/6095468164018077696/operations/293118014574821376\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MatchingEngineIndex deleted. . Resource name: projects/314481442207/locations/us-central1/indexes/6095468164018077696\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.base:MatchingEngineIndex deleted. . Resource name: projects/314481442207/locations/us-central1/indexes/6095468164018077696\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BucketNotFoundException: 404 gs://k-bucket-vertexai-service-project-vector-s-sf bucket does not exist.\n",
            "CommandException: Encountered non-existent bucket during listing\n",
            "\u001b[1;31mERROR:\u001b[0m (gcloud.redis.instances.delete) PERMISSION_DENIED: Google Cloud Memorystore for Redis API has not been used in project vertexai-service-project before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/redis.googleapis.com/overview?project=vertexai-service-project then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry.\n",
            "- '@type': type.googleapis.com/google.rpc.Help\n",
            "  links:\n",
            "  - description: Google developers console API activation\n",
            "    url: https://console.developers.google.com/apis/api/redis.googleapis.com/overview?project=vertexai-service-project\n",
            "- '@type': type.googleapis.com/google.rpc.ErrorInfo\n",
            "  domain: googleapis.com\n",
            "  metadata:\n",
            "    consumer: projects/vertexai-service-project\n",
            "    service: redis.googleapis.com\n",
            "  reason: SERVICE_DISABLED\n"
          ]
        }
      ],
      "source": [
        "# Force undeployment of indexes and delete endpoint\n",
        "my_index_endpoint.delete(force=True)\n",
        "\n",
        "# Delete indexes\n",
        "tree_ah_index.delete()\n",
        "\n",
        "# Delete cloud storage bucket\n",
        "delete_bucket = True\n",
        "if delete_bucket:\n",
        "    ! gsutil rm -rf {BUCKET_URI}\n",
        "\n",
        "# Delete redis instance\n",
        "! gcloud redis instances delete '{REDIS_INSTANCE_NAME}' --region {LOCATION} --quiet"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "sdk_vector_search_create_stack_overflow_embeddings.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
